\documentclass[13pt,a4paper]{article}
\usepackage[T1, T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsthm}
\usepackage[english,russian]{babel} 
\usepackage{graphicx}
\usepackage[left=2cm,right=1cm,
top=2cm,bottom=3cm,bindingoffset=0cm]{geometry} 
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{indentfirst}
\usepackage{hyperref}
\usepackage[normalem]{ulem}
\usepackage{xcolor}
\usepackage{soul}
\newtheorem{lemma}{Лемма}
\newtheorem{definition}{Определение}
\newtheorem{properties}{Свойства}

\newtheorem{theorem}{Теорема}
\newtheorem{method}{Метод}

\theoremstyle{remark}
\newtheorem*{remark}{Замечание}
\newtheorem*{mem}{Напоминание}

% \newcommand*\backmatter{%
%  \setcounter{section}{0}%
%  \renewcommand\section{back.\arabic{section}}}


\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclareMathOperator\arctanh{arctanh}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\const}{const}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\suml}{\sum\limits}
\newcommand\tab[1][1cm]{\hspace*{#1}}
\newcommand\given[1][]{\:#1\vert\:}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\renewcommand{\le}{\leqslant}
\renewcommand{\ge}{\geqslant}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\newcommand{\eps}{\varepsilon}
\renewcommand{\phi}{\varphi}

\graphicspath{ {images/} }

% Цвета для гиперссылок
\definecolor{linkcolor}{HTML}{799B03} % цвет ссылок
\definecolor{urlcolor}{HTML}{799B03} % цвет гиперссылок

\hypersetup{pdfstartview=FitH,  linkcolor=linkcolor,urlcolor=urlcolor, colorlinks=true}

\title{Легковесный детектор для токсичного текстового контента}
\author{Михаил Мартинсон}


\begin{document}
	\maketitle
	
	\label{firstpage}
	
	\subsection*{Задача}
	
	В этом задании требовалось обучить лекгий и быстрый классификатор для короткий текстов. Датасет состоит из кротких сообщений людей
	
	
	\href{https://github.com/MartinsonMichael/VK_Lab_intro/blob/master/%D0%9F%D1%80%D0%B8%D0%BA%D0%BB%D0%B0%D0%B4%D0%BD%D1%8B%D0%B5%20%D0%B8%D1%81%D1%81%D0%BB%D0%B5%D0%B4%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D1%8F%20.pdf}{Условие}
		
	Данные \href{https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/overview}{Kaggle. Toxic Comment Classification Challenge}
	
	Репозиторий решения \href{https://github.com/MartinsonMichael/VK_Lab_intro}{github.com/MartinsonMichael}
		
	Основной файл решения \href{https://github.com/MartinsonMichael/VK_Lab_intro/blob/master/nn_model_simple_rnn.ipynb}{nn\_model\_simple\_rnn.ipynb|}

	\subsection*{Инструкция по запуску}
	
	Предполпгается наличее pip'a и питона. 
	
	Уснановить зависимости \verb| pip install -r req.txt | в корне склонированного проекта.
	
	Зупустить \verb|nn_model_simple_rnn.ipynb| 
	
	
	\section{Моя модель}
	
	\subsection*{Эксперименты}
	
	Так как в задании есть бонусная часть про интерпретируемость модели, вначале я решил использовать в качестве основной модель основанную на деревьях. Немного эксперементов с Random Forest лежат в juputer-notebook
	\href{https://github.com/MartinsonMichael/VK_Lab_intro/blob/master/Trees.ipynb}{Trees.ipynb}. В эксперементах видно увеличение точности по мере роста глубины деревьев. Но модель так и не достигнув точности второй, нейронной модели, стала весить критично много (906BM для 1000 деревьев глубины 80).
	
	
	Следующий и финальный эксепемент был с простой нейроной архетикторуй.
	
	\subsection*{Финальная модель}
	
	Модель состоит из byte pair encoder, который переводит строку текста в последовательность токенов. Выбор этого токенизатора обусловлен с одной стороны возможностью держать в словаре целые слова и строить эмюеддинги для них, а с другой стороны способностью токенизировать любое слово.
	
	Далее для каждого токена строется эмбеддинг маленького размера, к кторому применяется Dence слой для попышения размерности. Это позволяет не создавать много весов для эмбеддингов, но в тоже время делать вычисляемый эмбеддинг слова не сильно маленьким.
	
	Следующий шаг это два слоя двунаправленного LSTM слоя. Во многих работах по языковым моделям говорится, что нижние слои таких последовательных RNN слоев имеют тенденцию улавливать более низкоуровневые, текстовые фичи, а более высокие - более сложные семантические фичи. Для данной задачи хочется как раз более высокоуровневых фичей, но в тоже время нельзя делать модель слишком громоздкой, поэтому слоев два.
	
	Далее еще один общий Dence слой и GlobalMaxPooling, чтобы привести выходы LSTM, которые могут быть разной длинны для разных предложений, к вектору фиксированной размерности. И финально еще два Dence слоя выдающие матрицу $ANS = [7, 2]$ для одного текста. Где $ANS[i, 1], i \in \overline{0, 5}$ вероятность тексту получить тег 
	
	\verb|['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']|
	
	А $ANS[6, i], i\in \{0, 1\}$ - дополнительно добавленный таргет, являющейся $1$, если хотя бы один из набора основных таргетов $1$. Предсказывание этого распределения по-идее должно помочь модели выделять признаки токсичных сообщений.


	\subsection*{Точность модели и Недостатки}
	
	Данные очень несбалансированные, $\sim 89 \%$ данный вообще не содержат ни одной метки. Поэтому метрика  accuracy, которую я считаю, почти ни о чем не говорит, хотя и является довольно высокой.
	
	
	
	\begin{center}
		\begin{tabular}{ c | c | c | c | c | c | c |}
			Лейбл & \verb|toxic| & \verb|severe_toxic| & \verb|obscene| & \verb|threat| & \verb|insult| & \verb|identity_hate| \\
			accuracy & 0.96 & 0.99 & 0.98 & 0.99 & 0.97 & 0.99 \\
			rocauc & 0.84 & 0.74 & 0.86 & 0.5 & 0.77 & 0.5 \\
		\end{tabular}
	\end{center}
	
	Среднее rocauc $0.72$
	
	
	Как можно видеть, rocauc для \verb|threat| и \verb|identity_hate| составляет $0.5$, что достигалось бы случайным классификатором. То есть модель так и не научилась выделять эти метки.
	
	
	\subsection*{Улучшения классификатора и ускорение}

	Прямое следствие предыдущего пункта, улучшение классификации \verb|threat| и \verb|identity_hate|. Для этого можно потренировать модель только на сбалансированной подвыборке этих двух классов. Или сделать дополнительные слои в 'головах' предсказывающих распределения этих двух классов.

	Почти все улучшения точности должны касаться либо обучения, либо архитектуры модели. Для архитектуры можно экспериментировать с токенизацией, так как для коротких текстов и легких классификаторов кажется важным как можно лучшем образом токенизировать текст. Также можно изменить механизм получения эмбеддингов, например взять уже предобученные эмбеддинги. Еще для уменьшения модели можно заменить тяжелые и большие матрицы из Dence слоев на их приближения разложениями.
	
	Если посмотреть на архитектуру нарисованную в юпитер-ноутбуке, то будет видно, что основной весовой вклад дают RNN слои. С ними связано возможное довольно сильное увеличение скорости. Так как в моделях работающих над последовательностями и имеющими внутри архитектуры RNN довольно долгой частью является работа рекуррентных блоков, так как их можно вычислять только строго последовательно. Поэтому возможно хорошим решением будет использование архитектур типа трансформера. В ней нет RNN блоков и она может хорошо параллелиться на несколько процессоров. Но для этого придется полностью изменить архитектуру модели.
	

	\section{Баланс между качеством и скоростью}
	
	 
	 
	 Глобально при выборе баланса между качеством и скоростью кажется хорошим решением, понять, что важнее, зафиксировать этот параметр и улучшать оставшийся. Отсюда мне кажется, что при выборе между легкой моделью, или оптимизацией большой, лучше выбрать легкую (конечно с достаточной capacity для решения задачи) и максимально хорошо ее обучить. Ибо при ускорении большой модели придется применить не меньше трюков, чем при обучении маленькой, но нужно еще и обучить большую.
	
	
	
%	\addcontentsline{toc}{section}{Список используемой литературы}
%	
%	%далее сам список используемой литературы
%	\begin{thebibliography}{}
%		\bibitem{word_embed_review} 
%		\href{http://ad-publications.informatik.uni-freiburg.de/theses/Bachelor_Jon_Ezeiza_2017.pdf}{(en) A review of word
%			embedding and document similarity algorithms applied to academic text.} (про Vector Space Mode, Deep Learning, Word embeddings,  review of word embedding algorithms (Word2Vec, GloVe, FastText, WordRank), review of document similarity measures (Baseline: VSM, Doc2Vec, Doc2VecC, Word Mover’s Distance, Skip-thoughts, Sent2Vec)), 90 листов.
%		
%		\bibitem{review_exp_compare_text_cluster} 
%		\href{http://www.mathnet.ru/links/2a50b93bfe9152511e398d666cc65d1b/tisp214.pdf}{(ru) Обзор и экспериментальное сравнение методов кластеризации текстов.} Хороший материал для начальной навигации.
%		
%		\bibitem{HB1} 
%		\href{https://habr.com/post/324686/}{(ru) Технологический стек классификации текстов на естественных языках.} Описаны сигнатурный и шаблонный классификаторы, склейка в мультикласс.
%		
%		\bibitem{Stacking}
%		\href{http://www.machinelearning.ru/wiki/images/5/56/Guschin2015Stacking.pdf}{(ru) Методы ансамблирования обучающихся алгоритмов}
%		
%		
%		
%	\end{thebibliography}
			
		\end{document}