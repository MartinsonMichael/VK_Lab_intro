{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "import scipy.stats as sts\n",
    "import bpe\n",
    "\n",
    "import keras as K\n",
    "import keras.layers as L\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "from IPython.display import HTML, display_html\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загружаем подготовленные данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pickle.load(open('./processed/rnn_features.pkl', 'rb'))\n",
    "labels = pd.read_csv('./processed/labels.csv')\n",
    "\n",
    "bpe_encoder = pickle.load(open('./processed/pbe_encoder.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Делим дданные на трейн для обучения и отложенную тестовую выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(features, labels.values, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция построения модели\n",
    "\n",
    "Модель должна быть легкой и быстрой, поэтому поставлю себе ограничение на миллион параметров и возможность применять модель для $20-30$ текстов в секунду.\n",
    "\n",
    "\n",
    "Обучать модель будем адамом с дефолтными параметрами, лосс - котегориальная кросэнтропия. Для этой функции потерь преопразуем предсказываемый вектор из размерности $shape=(7, )$, в вектор $shape=(7, 2)$ где позиция $[i, j], j \\in \\{0, 1\\}$ таргета будет вероятностью $i$'ой позиции таргета быть $j$\n",
    "\n",
    "Более детальное описание модели в отчете. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    l_input = L.Input(shape=(None, ))\n",
    "    l_in2 = L.Embedding(input_dim=bpe_encoder.vocab_size, output_dim=10)(l_input)\n",
    "    \n",
    "    # (batch, len, 10) -> (batch, len, 128)\n",
    "    l_in3 = L.TimeDistributed(L.Dense(units=128))(l_in2)\n",
    "    \n",
    "    # (batch, len, 128) -> (batch, len, 256)\n",
    "    l_rnn1 = L.Bidirectional(L.LSTM(units=128, return_sequences=True))(l_in3)\n",
    "    # (batch, len, 256) -> (batch, len, 256)\n",
    "    l_rnn2 = L.Bidirectional(L.LSTM(units=128, return_sequences=True))(l_rnn1)\n",
    "    \n",
    "    # (batch, len, 256) -> (batch, len, 128)\n",
    "    l_dense1 = L.TimeDistributed(L.Dense(units=128))(l_rnn2)\n",
    "    # (batch, len, 128) -> (batch, 128)\n",
    "    l_comb = L.GlobalMaxPool1D()(l_dense1)\n",
    "    # (batch, 128) -> (batch, 128)\n",
    "    l_dence2 = L.Dense(units=128, activation='relu')(l_comb)\n",
    "    # (batch, 128) -> (batch, 14)\n",
    "    l_final = L.Dense(units=2 * 7)(l_dence2)\n",
    "    # (batch, 14) -> (batch, 7, 2)\n",
    "    l_final_reshape = L.Reshape(target_shape=(7, 2))(l_final)\n",
    "    # (batch, 7, 2) -> (batch, 7, 2)\n",
    "    l_prob = L.Softmax(axis=2)(l_final_reshape)\n",
    "    \n",
    "    model = K.Model(input=l_input, output=l_prob)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используйте одну из следующих двух ячеек чтобы сделать новую модель, или загрузить модель из файла.\n",
    "\n",
    "*В репозитории на github должна лежать самая хорошая модель*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0929 22:17:03.279319 140483696359232 deprecation_wrapper.py:119] From /home/michael/.virtualenv/DS3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0929 22:17:03.314570 140483696359232 deprecation_wrapper.py:119] From /home/michael/.virtualenv/DS3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0929 22:17:03.318742 140483696359232 deprecation_wrapper.py:119] From /home/michael/.virtualenv/DS3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "/home/michael/.virtualenv/DS3.6/lib/python3.6/site-packages/ipykernel_launcher.py:26: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"so...)`\n",
      "W0929 22:17:04.463596 140483696359232 deprecation_wrapper.py:119] From /home/michael/.virtualenv/DS3.6/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0929 22:17:04.483611 140483696359232 deprecation_wrapper.py:119] From /home/michael/.virtualenv/DS3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./models/model4_30.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_20 (InputLayer)        (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_20 (Embedding)     (None, None, 10)          81920     \n",
      "_________________________________________________________________\n",
      "time_distributed_32 (TimeDis (None, None, 128)         1408      \n",
      "_________________________________________________________________\n",
      "bidirectional_33 (Bidirectio (None, None, 256)         263168    \n",
      "_________________________________________________________________\n",
      "bidirectional_34 (Bidirectio (None, None, 256)         394240    \n",
      "_________________________________________________________________\n",
      "time_distributed_33 (TimeDis (None, None, 128)         32896     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_15 (Glo (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 14)                1806      \n",
      "_________________________________________________________________\n",
      "reshape_14 (Reshape)         (None, 7, 2)              0         \n",
      "_________________________________________________________________\n",
      "softmax_14 (Softmax)         (None, 7, 2)              0         \n",
      "=================================================================\n",
      "Total params: 791,950\n",
      "Trainable params: 791,950\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получилась довольно компактная модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию - генератор батчей для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_batch1(X, Y, batch_size=6, smooth=0.2, unk_prob=0.07):\n",
    "    \n",
    "    # сохраним значение токенов Паддинга (pad) и Неизвестного токена (unk)\n",
    "    UNK_num = list(bpe_encoder.transform([bpe_encoder.UNK]))[0][0]\n",
    "    PAD_num = list(bpe_encoder.transform([bpe_encoder.PAD]))[0][0]\n",
    "    \n",
    "    print(f'unk: {UNK_num}, pad: {PAD_num}')\n",
    "    \n",
    "    # Отсортируем наши данные по количеству токенов в тектсте\n",
    "    buf = sorted([(len(x), x, y) for (x, y) in zip(X, Y) ], key=lambda x: x[0])\n",
    "    X, Y = zip(*[(x, y) for _, x, y in buf ])\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    \n",
    "    while True:\n",
    "        # далее сохраним возможные индексы начала батча и отсортируем их\n",
    "        # батч будем формировать беря очередной индекс и следующие batch_size\n",
    "        #   элементов за ним\n",
    "        # таким образом мы получим случайную последовательность проходящую по всем\n",
    "        #   элементам выборки, и в добавок каждый батч будет содержать примерно равные\n",
    "        #   по длинне последовательности токенов \n",
    "        indexes = np.arange(len(X) - batch_size)\n",
    "        np.random.shuffle(indexes)\n",
    "        \n",
    "        for ind in indexes:\n",
    "            # делаем one-hot encoding таргета\n",
    "            y = to_categorical(np.expand_dims(Y[ind : ind + batch_size, :], axis=2), 2)\n",
    "            \n",
    "            # применяем смуфинг, этот прием помогает делать сеть не такой уверенной в своих\n",
    "            #   выводах, что хорошо влияет на метрики на отложенной выборке\n",
    "            y[y == 1] = 1 - smooth\n",
    "            y[y == 0] = smooth\n",
    "            \n",
    "            # дополняем все последовательности текущего батча до макимальной длинны,\n",
    "            #   заполняя короткие последовательности Паддингами\n",
    "            x = pad_sequences(X[ind : ind + batch_size], padding='post', value=PAD_num)\n",
    "            \n",
    "            # еще один небольшой хак, давайте с некоторой вероятностью заменять реальные\n",
    "            #   токены на Неизвестный токен - unk, интуитивное объяснение этому, в том что\n",
    "            #   сеть начинает делать предсказания в условии меньшей информации и меньше\n",
    "            #   полагается на конкретные слова, так как они могут 'выпасть' \n",
    "            len_min = len(X[ind])\n",
    "            x_unk = np.random.binomial(1, unk_prob, (batch_size, len_min))\n",
    "            x[:, :len_min][x_unk == 1] = UNK_num\n",
    "            \n",
    "            # и наконец отправим батч с сетку\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем сохранять промежуточныйе модели по ходу обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(\n",
    "    filepath=r\"./models/model_{epoch:02d}.hdf5\", \n",
    "    save_best_only=False,\n",
    "    save_weights_only=False,\n",
    "    period=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "реально это клеточка запусклась 4 раза, так что при запуске на только что инициализированной модели, значения лосса будут чуть выше, примерно 0.58 в среднем за эпоху"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "unk: 1, pad: 0\n",
      "500/500 [==============================] - 97s 194ms/step - loss: 0.5159\n",
      "Epoch 2/30\n",
      "500/500 [==============================] - 102s 203ms/step - loss: 0.5157\n",
      "Epoch 3/30\n",
      "500/500 [==============================] - 97s 194ms/step - loss: 0.5168\n",
      "Epoch 4/30\n",
      "500/500 [==============================] - 112s 224ms/step - loss: 0.5143\n",
      "Epoch 5/30\n",
      "500/500 [==============================] - 105s 209ms/step - loss: 0.5158\n",
      "Epoch 6/30\n",
      "500/500 [==============================] - 99s 198ms/step - loss: 0.5140\n",
      "Epoch 7/30\n",
      "500/500 [==============================] - 93s 186ms/step - loss: 0.5138\n",
      "Epoch 8/30\n",
      "500/500 [==============================] - 110s 221ms/step - loss: 0.5147\n",
      "Epoch 9/30\n",
      "500/500 [==============================] - 97s 194ms/step - loss: 0.5145\n",
      "Epoch 10/30\n",
      "500/500 [==============================] - 116s 232ms/step - loss: 0.5150\n",
      "Epoch 11/30\n",
      "500/500 [==============================] - 111s 223ms/step - loss: 0.5153\n",
      "Epoch 12/30\n",
      "500/500 [==============================] - 111s 222ms/step - loss: 0.5145\n",
      "Epoch 13/30\n",
      "500/500 [==============================] - 102s 204ms/step - loss: 0.5153\n",
      "Epoch 14/30\n",
      "500/500 [==============================] - 118s 236ms/step - loss: 0.5138\n",
      "Epoch 15/30\n",
      "500/500 [==============================] - 104s 208ms/step - loss: 0.5136\n",
      "Epoch 16/30\n",
      "500/500 [==============================] - 102s 204ms/step - loss: 0.5133\n",
      "Epoch 17/30\n",
      "500/500 [==============================] - 114s 227ms/step - loss: 0.5122\n",
      "Epoch 18/30\n",
      "500/500 [==============================] - 107s 215ms/step - loss: 0.5137\n",
      "Epoch 19/30\n",
      "500/500 [==============================] - 95s 189ms/step - loss: 0.5144\n",
      "Epoch 20/30\n",
      "500/500 [==============================] - 102s 205ms/step - loss: 0.5148\n",
      "Epoch 21/30\n",
      "500/500 [==============================] - 101s 203ms/step - loss: 0.5152\n",
      "Epoch 22/30\n",
      "500/500 [==============================] - 109s 217ms/step - loss: 0.5137\n",
      "Epoch 23/30\n",
      "500/500 [==============================] - 108s 216ms/step - loss: 0.5139\n",
      "Epoch 24/30\n",
      "500/500 [==============================] - 100s 199ms/step - loss: 0.5134\n",
      "Epoch 25/30\n",
      "500/500 [==============================] - 111s 223ms/step - loss: 0.5140\n",
      "Epoch 26/30\n",
      "500/500 [==============================] - 120s 239ms/step - loss: 0.5132\n",
      "Epoch 27/30\n",
      "500/500 [==============================] - 109s 219ms/step - loss: 0.5129\n",
      "Epoch 28/30\n",
      "500/500 [==============================] - 92s 184ms/step - loss: 0.5137\n",
      "Epoch 29/30\n",
      "500/500 [==============================] - 108s 216ms/step - loss: 0.5135\n",
      "Epoch 30/30\n",
      "500/500 [==============================] - 103s 206ms/step - loss: 0.5134\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0d5ba14fd0>"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    generator=generator_batch1(x_train, y_train),\n",
    "    steps_per_epoch=500,\n",
    "    epochs=30,\n",
    "    callbacks=[checkpointer]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Посчитаем метрики модели\n",
    "\n",
    "Данные очень несбалансированные, $\\sim 89 \\%$ данный вообще не содержат ни одной метки. Поэтому метрика точности (accuracy), которую я считаю, почти ни о чем не говорит. Так что я счиатаю roc auc, но и accuracy тоже, просто она мне нравится."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 23s, sys: 16.4 s, total: 4min 39s\n",
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "predictions = np.array(list(map(\n",
    "    lambda x: np.argmax(model.predict(np.array([x]))[0], axis=1),\n",
    "    x_test[:5000],\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic\n",
      "accuracy: 0.9686\n",
      "rocauc: 0.8721912068591522\n",
      "\n",
      "severe_toxic\n",
      "accuracy: 0.9906\n",
      "rocauc: 0.7255538192848607\n",
      "\n",
      "obscene\n",
      "accuracy: 0.9846\n",
      "rocauc: 0.914971079033579\n",
      "\n",
      "threat\n",
      "accuracy: 0.9974\n",
      "rocauc: 0.5\n",
      "\n",
      "insult\n",
      "accuracy: 0.973\n",
      "rocauc: 0.8274095715911618\n",
      "\n",
      "identity_hate\n",
      "accuracy: 0.9904\n",
      "rocauc: 0.5\n",
      "\n",
      "toxic_content\n",
      "accuracy: 0.969\n",
      "rocauc: 0.8832260068914872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels_column = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'] + ['toxic_content']\n",
    "\n",
    "for i in range(7):\n",
    "    print(labels_column[i])\n",
    "    print('accuracy', end=': ')\n",
    "    print(accuracy_score(predictions[:, i], y_test[:5000, i]))\n",
    "    print('rocauc', end=': ')\n",
    "    print(roc_auc_score(y_test[:5000, i], predictions[:, i]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "предсказания по всему"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7461930976657486"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test[:5000], predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "без искуственно добавленной метки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7233542794614589"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test[:5000, :6], predictions[:, :6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Немного поанализируем модель\n",
    "\n",
    "\n",
    "Попытка сделать бонусное задание по интропретируемости.\n",
    "\n",
    "Посмотрим, на что модельно обращает внимание больше, а на что меньше. Для этого есть инересный подход, заменять последовательно слова на UNK и смотреть, как меняется оцнека моделью вероятностей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# всмомогательная функция рисования цветных слов\n",
    "# ПИСАЛ НЕ САМ! Взято из второго семинара курса ШАДа по NLP читавшегося осенью 2018 года.\n",
    "\n",
    "def draw_html(tokens_and_weights, cmap=plt.get_cmap(\"bwr\"), display=True,\n",
    "              token_template=\"\"\"<span style=\"background-color: {color_hex}\">{token}</span>\"\"\",\n",
    "              font_style=\"font-size:14px;\"\n",
    "             ):\n",
    "    \n",
    "    def get_color_hex(weight):\n",
    "        rgba = cmap(1. / (1 + np.exp(weight)), bytes=True)\n",
    "        return '#%02X%02X%02X' % rgba[:3]\n",
    "    \n",
    "    tokens_html = [\n",
    "        token_template.format(token=token, color_hex=get_color_hex(weight))\n",
    "        for token, weight in tokens_and_weights\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    raw_html = \"\"\"<p style=\"{}\">{}</p>\"\"\".format(font_style, ' '.join(tokens_html))\n",
    "    if display:\n",
    "        display_html(HTML(raw_html))\n",
    "        \n",
    "    return raw_html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь сам напишу функцию, которая будет считать вклады слов в итоговый скор.\n",
    "\n",
    "Функция принимает модель, токены и функцю извлечения нужной метрики из предсказания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wieght(model, tokens, weight_extracter):\n",
    "    # посчитает базовый скор, без унков\n",
    "    base_score = weight_extracter(model.predict(np.array([tokens]))[0])\n",
    "    \n",
    "    # сохраняем Токен унка\n",
    "    UNK_num = list(bpe_encoder.transform([bpe_encoder.UNK]))[0][0]\n",
    "    \n",
    "    # веса\n",
    "    weights = []\n",
    "    position = 0\n",
    "    \n",
    "    while position < len(tokens):\n",
    "        token = tokens[position]\n",
    "        \n",
    "        # если слово\n",
    "        if token in bpe_encoder.inverse_bpe_vocab.keys():\n",
    "            infer = process_tokenized_word(model, tokens, position, base_score, weight_extracter, UNK_num)\n",
    "        \n",
    "        # если нечто токенизированное\n",
    "        if token in bpe_encoder.inverse_word_vocab.keys():\n",
    "            infer = process_single_word(model, tokens, position, base_score, weight_extracter, UNK_num)\n",
    "            \n",
    "        weight, word, positino_up = infer\n",
    "        \n",
    "        weights.append((word, weight))\n",
    "        position += positino_up\n",
    "        \n",
    "    return weights, base_score\n",
    "\n",
    "def process_single_word(model, tokens, position, base_score, weight_extracter, UNK):\n",
    "    buf = tokens.copy()\n",
    "    buf[position] = UNK\n",
    "\n",
    "    # считаем вклад токена\n",
    "    predictions = model.predict(np.array([buf]))[0]\n",
    "    weight = weight_extracter(predictions) - base_score\n",
    "    \n",
    "    return (weight, bpe_encoder.inverse_word_vocab[tokens[position]], 1)\n",
    "\n",
    "def process_tokenized_word(model, tokens, position, base_score, weight_extracter, UNK):\n",
    "    # считаем вклад токенезированного слова как сумму вкладов его частей\n",
    "    sm = 0\n",
    "    for i in range(position, len(tokens)):\n",
    "        buf = tokens.copy()\n",
    "        buf[i] = UNK\n",
    "        \n",
    "        # считаем вклад токена\n",
    "        predictions = model.predict(np.array([buf]))[0]\n",
    "        weight = weight_extracter(predictions)\n",
    "        sm += weight - base_score\n",
    "        \n",
    "        # если токен был концом слова, вернем вес, само слово и новую позицию\n",
    "        if tokens[i] == bpe_encoder.bpe_vocab['__eow']:\n",
    "            word = next(bpe_encoder.inverse_transform([tokens[position : i + 1]]))\n",
    "            return (sm, word, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Понаслаждаемся, раскрасим некоторые предложения по влиянию на токсичность\n",
    "\n",
    "\n",
    "К сожалению цветовые теги не отображаются в просмоторщике github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drow_tokcik(tokens, realy=None):\n",
    "    tokens_and_weights, base_score = get_wieght(model, tokens, lambda x: x[0][1])\n",
    "    print(f'It was tixic for { str(round(base_score, 4))[:4] }')\n",
    "    if realy is not None:\n",
    "        print(f'And realy {realy[0]}')\n",
    "    draw_html([(tok, weight * 100) for tok, weight in tokens_and_weights]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was tixic for 0.20\n",
      "And realy 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"font-size:14px;\"><span style=\"background-color: #FCFCFF\">please</span> <span style=\"background-color: #C2C2FF\">undo</span> <span style=\"background-color: #F8F8FF\">your</span> <span style=\"background-color: #F0F0FF\">second</span> <span style=\"background-color: #F8F8FF\">reversion</span> <span style=\"background-color: #F8F8FF\">of</span> <span style=\"background-color: #FEFEFF\">my</span> <span style=\"background-color: #E6E6FF\">removal</span> <span style=\"background-color: #F3F3FF\">of</span> <span style=\"background-color: #FFFEFE\">that</span> <span style=\"background-color: #F6F6FF\">taxobox</span> <span style=\"background-color: #FF3636\">gn is</span></p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drow_tokcik(x_test[1], y_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was tixic for 0.22\n",
      "And realy 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"font-size:14px;\"><span style=\"background-color: #6060FF\">fun</span> <span style=\"background-color: #FF7E7E\">!!!</span> <span style=\"background-color: #6464FF\">lol</span> <span style=\"background-color: #FFE0E0\">jk</span> <span style=\"background-color: #FF2020\">ts</span></p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drow_tokcik(x_test[180], y_test[180])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was tixic for 0.2\n",
      "And realy 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"font-size:14px;\"><span style=\"background-color: #EEEEFF\">,</span> <span style=\"background-color: #FFACAC\">you</span> <span style=\"background-color: #FFE2E2\">don</span> <span style=\"background-color: #FFEAEA\">'</span> <span style=\"background-color: #FFEAEA\">t</span> <span style=\"background-color: #FFFCFC\">call</span> <span style=\"background-color: #EEEEFF\">respected</span> <span style=\"background-color: #8080FF\">review</span> <span style=\"background-color: #ECECFF\">sites</span> <span style=\"background-color: #D3D3FF\">numrd</span> <span style=\"background-color: #FEFEFF\">party</span> <span style=\"background-color: #E0E0FF\">coverage</span> <span style=\"background-color: #FFFEFE\">?</span></p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drow_tokcik(x_test[12], y_test[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was tixic for 0.19\n",
      "And realy 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"font-size:14px;\"><span style=\"background-color: #F3F3FF\">posting</span> <span style=\"background-color: #FFFAFA\">styles</span> <span style=\"background-color: #FFFCFC\">.</span> <span style=\"background-color: #FFFEFE\">.</span> <span style=\"background-color: #FEFEFF\">that</span> <span style=\"background-color: #FFFCFC\">is</span> <span style=\"background-color: #FFEEEE\">woefully</span> <span style=\"background-color: #FFFCFC\">ie</span> <span style=\"background-color: #FFFCFC\">nt</span> <span style=\"background-color: #FFFCFC\"></span> <span style=\"background-color: #7171FF\">context</span> <span style=\"background-color: #FAFAFF\">here</span> <span style=\"background-color: #FFE6E6\">.</span></p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drow_tokcik(x_test[186], y_test[186])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was tixic for 0.21\n",
      "And realy 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"font-size:14px;\"><span style=\"background-color: #FFB0B0\">acs</span> <span style=\"background-color: #FFEEEE\"></span> <span style=\"background-color: #FFA0A0\">are</span> <span style=\"background-color: #FFE8E8\">either</span> <span style=\"background-color: #FCFCFF\">another</span> <span style=\"background-color: #FFFEFE\">blackjack</span> <span style=\"background-color: #FFFEFE\"></span> <span style=\"background-color: #FFFEFE\">di</span> <span style=\"background-color: #FEFEFF\">sa</span> <span style=\"background-color: #FEFEFF\">bi</span> <span style=\"background-color: #F3F3FF\">li ty</span> <span style=\"background-color: #FFFEFE\">the</span> <span style=\"background-color: #FEFEFF\">other</span> <span style=\"background-color: #FFFEFE\">hand</span> <span style=\"background-color: #FFFEFE\">are</span> <span style=\"background-color: #FEFEFF\">part</span> <span style=\"background-color: #FEFEFF\">of</span> <span style=\"background-color: #FEFEFF\">a</span> <span style=\"background-color: #FEFEFF\">community</span> <span style=\"background-color: #FEFEFF\">of</span> <span style=\"background-color: #FEFEFF\">anonymous</span> <span style=\"background-color: #FEFEFF\">creeps</span></p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drow_tokcik(x_test[34], y_test[34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was tixic for 0.20\n",
      "And realy 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"font-size:14px;\"><span style=\"background-color: #FFC6C6\">tu</span> <span style=\"background-color: #FFEAEA\"></span> <span style=\"background-color: #FFFAFA\">an</span> <span style=\"background-color: #FCFCFF\">article</span> <span style=\"background-color: #FFFEFE\">here</span> <span style=\"background-color: #FEFEFF\">that</span> <span style=\"background-color: #FFFEFE\">the</span> <span style=\"background-color: #FEFEFF\">budget</span> <span style=\"background-color: #FCFCFF\">dispute</span> <span style=\"background-color: #FEFEFF\">is</span> <span style=\"background-color: #FEFEFF\">going</span> <span style=\"background-color: #FFFEFE\">to</span> <span style=\"background-color: #FEFEFF\">court</span> <span style=\"background-color: #FFFEFE\">now</span> <span style=\"background-color: #FEFEFF\">.</span> <span style=\"background-color: #FEFEFF\">i</span> <span style=\"background-color: #FFFEFE\">didnt</span> <span style=\"background-color: #FEFEFF\">read</span> <span style=\"background-color: #FEFEFF\">the</span> <span style=\"background-color: #FEFEFF\">specifics</span> <span style=\"background-color: #FEFEFF\">though</span> <span style=\"background-color: #FEFEFF\">.</span> <span style=\"background-color: #FFFEFE\">that</span> <span style=\"background-color: #FEFEFF\">is</span> <span style=\"background-color: #FEFEFF\">really</span> <span style=\"background-color: #FEFEFF\">cool</span> <span style=\"background-color: #FFFEFE\">you</span> <span style=\"background-color: #FFFEFE\">got</span> <span style=\"background-color: #FFFEFE\">that</span> <span style=\"background-color: #FEFEFF\">photo</span> <span style=\"background-color: #FEFEFF\">!</span> <span style=\"background-color: #FFFEFE\">any</span> <span style=\"background-color: #FEFEFF\">word</span> <span style=\"background-color: #FEFEFF\">on</span> <span style=\"background-color: #FFFEFE\">when</span> <span style=\"background-color: #FEFEFF\">the</span> <span style=\"background-color: #FEFEFF\">new</span> <span style=\"background-color: #FEFEFF\">city</span> <span style=\"background-color: #FFFEFE\">hall</span> <span style=\"background-color: #FEFEFF\">/</span> <span style=\"background-color: #FFFEFE\">old</span> <span style=\"background-color: #FFFEFE\">verizon</span> <span style=\"background-color: #FFC0C0\">stuck</span> <span style=\"background-color: #FFF2F2\">it</span> <span style=\"background-color: #FFFAFA\">...</span> <span style=\"background-color: #FEFEFF\">i</span> <span style=\"background-color: #FFFEFE\">seem</span> <span style=\"background-color: #FEFEFF\">to</span> <span style=\"background-color: #FEFEFF\">have</span> <span style=\"background-color: #FFFEFE\">been</span> <span style=\"background-color: #F8F8FF\">distracted</span></p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drow_tokcik(x_test[157], y_test[157])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иногда модель обращает внимание на какой-то бред :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Эксперементы быстродействия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./data/train.csv')\n",
    "texts = list(train_data.comment_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_prediction(text_line, model):\n",
    "    tokens = next(bpe_encoder.transform([text_line]))\n",
    "    prediction = np.argmax(model.predict(np.array([tokens]))[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 34min 11s, sys: 5min 52s, total: 1h 40min 3s\n",
      "Wall time: 31min 3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " ...]"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "list(map(lambda x : get_single_prediction(x, model), texts[:100 * 1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это заняло $94$ минуты процессорного времени. То есть $57ms$ на полный процессинг одного текста. Что кажется довольно быстро, учитывая возможность процессить батчами.\n",
    "\n",
    "*Во время всего обучения и процессингов использовались CPU: 4 ядра Intel Core5*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05651"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(94 * 60 + 11) / 100 / 1000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
