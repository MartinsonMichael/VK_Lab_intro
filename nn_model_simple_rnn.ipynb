{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "import scipy.stats as sts\n",
    "import bpe\n",
    "\n",
    "import keras as K\n",
    "import keras.layers as L\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from IPython.display import HTML, display_html\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загружаем подготовленные данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pickle.load(open('./processed/rnn_features.pkl', 'rb'))\n",
    "labels = pd.read_csv('./processed/labels.csv')\n",
    "\n",
    "bpe_encoder = pickle.load(open('./processed/pbe_encoder.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Делим дданные на трейн для обучения и отложенную тестовую выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(features, labels.values, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция построения модели\n",
    "\n",
    "Модель должна быть легкой и быстрой, поэтому поставлю себе ограничение на миллион параметров и возможность применять модель для $20-30$ текстов в секунду.\n",
    "\n",
    "\n",
    "Обучать модель будем адамом с дефолтными параметрами, лосс - котегориальная кросэнтропия. Для этой функции потерь преопразуем предсказываемый вектор из размерности $shape=(7, )$, в вектор $shape=(7, 2)$ где позиция $[i, j], j \\in \\{0, 1\\}$ таргета будет вероятностью $i$'ой позиции таргета быть $j$\n",
    "\n",
    "Более детальное описание модели и способа обучения в отчете. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    l_input = L.Input(shape=(None, ))\n",
    "    l_in2 = L.Embedding(input_dim=bpe_encoder.vocab_size, output_dim=10)(l_input)\n",
    "    \n",
    "    # (batch, len, 10) -> (batch, len, 128)\n",
    "    l_in3 = L.TimeDistributed(L.Dense(units=128))(l_in2)\n",
    "    \n",
    "    # (batch, len, 128) -> (batch, len, 256)\n",
    "    l_rnn1 = L.Bidirectional(L.LSTM(units=128, return_sequences=True))(l_in3)\n",
    "    # (batch, len, 256) -> (batch, len, 256)\n",
    "    l_rnn2 = L.Bidirectional(L.LSTM(units=128, return_sequences=True))(l_rnn1)\n",
    "    \n",
    "    # (batch, len, 256) -> (batch, len, 128)\n",
    "    l_dense1 = L.TimeDistributed(L.Dense(units=128))(l_rnn2)\n",
    "    # (batch, len, 128) -> (batch, 128)\n",
    "    l_comb = L.GlobalMaxPool1D()(l_dense1)\n",
    "    # (batch, 128) -> (batch, 128)\n",
    "    l_dence2 = L.Dense(units=128, activation='relu')(l_comb)\n",
    "    # (batch, 128) -> (batch, 14)\n",
    "    l_final = L.Dense(units=2 * 7)(l_dence2)\n",
    "    # (batch, 14) -> (batch, 7, 2)\n",
    "    l_final_reshape = L.Reshape(target_shape=(7, 2))(l_final)\n",
    "    # (batch, 7, 2) -> (batch, 7, 2)\n",
    "    l_prob = L.Softmax(axis=2)(l_final_reshape)\n",
    "    \n",
    "    model = K.Model(input=l_input, output=l_prob)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/.virtualenv/DS3.6/lib/python3.6/site-packages/ipykernel_launcher.py:26: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"so...)`\n"
     ]
    }
   ],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_20 (InputLayer)        (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_20 (Embedding)     (None, None, 10)          81920     \n",
      "_________________________________________________________________\n",
      "time_distributed_32 (TimeDis (None, None, 128)         1408      \n",
      "_________________________________________________________________\n",
      "bidirectional_33 (Bidirectio (None, None, 256)         263168    \n",
      "_________________________________________________________________\n",
      "bidirectional_34 (Bidirectio (None, None, 256)         394240    \n",
      "_________________________________________________________________\n",
      "time_distributed_33 (TimeDis (None, None, 128)         32896     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_15 (Glo (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 14)                1806      \n",
      "_________________________________________________________________\n",
      "reshape_14 (Reshape)         (None, 7, 2)              0         \n",
      "_________________________________________________________________\n",
      "softmax_14 (Softmax)         (None, 7, 2)              0         \n",
      "=================================================================\n",
      "Total params: 791,950\n",
      "Trainable params: 791,950\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получилась довольно компактная модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию - генератор батчей для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_batch1(X, Y, batch_size=6, smooth=0.2, unk_prob=0.07):\n",
    "    \n",
    "    # сохраним значение токенов Паддинга (pad) и Неизвестного токена (unk)\n",
    "    UNK_num = list(bpe_encoder.transform([bpe_encoder.UNK]))[0][0]\n",
    "    PAD_num = list(bpe_encoder.transform([bpe_encoder.PAD]))[0][0]\n",
    "    \n",
    "    print(f'unk: {UNK_num}, pad: {PAD_num}')\n",
    "    \n",
    "    # Отсортируем наши данные по количеству токенов в тектсте\n",
    "    buf = sorted([(len(x), x, y) for (x, y) in zip(X, Y) ], key=lambda x: x[0])\n",
    "    X, Y = zip(*[(x, y) for _, x, y in buf ])\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    \n",
    "    while True:\n",
    "        # далее сохраним возможные индексы начала батча и отсортируем их\n",
    "        # батч будем формировать беря очередной индекс и следующие batch_size\n",
    "        #   элементов за ним\n",
    "        # таким образом мы получим случайную последовательность проходящую по всем\n",
    "        #   элементам выборки, и в добавок каждый батч будет содержать примерно равные\n",
    "        #   по длинне последовательности токенов \n",
    "        indexes = np.arange(len(X) - batch_size)\n",
    "        np.random.shuffle(indexes)\n",
    "        \n",
    "        for ind in indexes:\n",
    "            # делаем one-hot encoding таргета\n",
    "            y = to_categorical(np.expand_dims(Y[ind : ind + batch_size, :], axis=2), 2)\n",
    "            \n",
    "            # применяем смуфинг, этот прием помогает делать сеть не такой уверенной в своих\n",
    "            #   выводах, что хорошо влияет на метрики на отложенной выборке\n",
    "            y[y == 1] = 1 - smooth\n",
    "            y[y == 0] = smooth\n",
    "            \n",
    "            # дополняем все последовательности текущего батча до макимальной длинны,\n",
    "            #   заполняя короткие последовательности Паддингами\n",
    "            x = pad_sequences(X[ind : ind + batch_size], padding='post', value=PAD_num)\n",
    "            \n",
    "            # еще один небольшой хак, давайте с некоторой вероятностью заменять реальные\n",
    "            #   токены на Неизвестный токен - unk, интуитивное объяснение этому, в том что\n",
    "            #   сеть начинает делать предсказания в условии меньшей информации и меньше\n",
    "            #   полагается на конкретные слова, так как они могут 'выпасть' \n",
    "            len_min = len(X[ind])\n",
    "            x_unk = np.random.binomial(1, unk_prob, (batch_size, len_min))\n",
    "            x[:, :len_min][x_unk == 1] = UNK_num\n",
    "            \n",
    "            # и наконец отправим батч с сетку\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем сохранять промежуточныйе модели по ходу обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(\n",
    "    filepath=r\"./models/model4_{epoch:02d}.hdf5\", \n",
    "    save_best_only=False,\n",
    "    save_weights_only=False,\n",
    "    period=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "реально это клеточка запусклась 4 раза, так что при запуске на только что инициализированной модели, значения лосса будут чуть выше, примерно 0.58 в среднем за эпоху"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "unk: 1, pad: 0\n",
      "500/500 [==============================] - 97s 194ms/step - loss: 0.5159\n",
      "Epoch 2/30\n",
      "500/500 [==============================] - 102s 203ms/step - loss: 0.5157\n",
      "Epoch 3/30\n",
      "500/500 [==============================] - 97s 194ms/step - loss: 0.5168\n",
      "Epoch 4/30\n",
      "500/500 [==============================] - 112s 224ms/step - loss: 0.5143\n",
      "Epoch 5/30\n",
      "500/500 [==============================] - 105s 209ms/step - loss: 0.5158\n",
      "Epoch 6/30\n",
      "500/500 [==============================] - 99s 198ms/step - loss: 0.5140\n",
      "Epoch 7/30\n",
      "500/500 [==============================] - 93s 186ms/step - loss: 0.5138\n",
      "Epoch 8/30\n",
      "500/500 [==============================] - 110s 221ms/step - loss: 0.5147\n",
      "Epoch 9/30\n",
      "500/500 [==============================] - 97s 194ms/step - loss: 0.5145\n",
      "Epoch 10/30\n",
      "500/500 [==============================] - 116s 232ms/step - loss: 0.5150\n",
      "Epoch 11/30\n",
      "500/500 [==============================] - 111s 223ms/step - loss: 0.5153\n",
      "Epoch 12/30\n",
      "500/500 [==============================] - 111s 222ms/step - loss: 0.5145\n",
      "Epoch 13/30\n",
      "500/500 [==============================] - 102s 204ms/step - loss: 0.5153\n",
      "Epoch 14/30\n",
      "500/500 [==============================] - 118s 236ms/step - loss: 0.5138\n",
      "Epoch 15/30\n",
      "500/500 [==============================] - 104s 208ms/step - loss: 0.5136\n",
      "Epoch 16/30\n",
      "500/500 [==============================] - 102s 204ms/step - loss: 0.5133\n",
      "Epoch 17/30\n",
      "500/500 [==============================] - 114s 227ms/step - loss: 0.5122\n",
      "Epoch 18/30\n",
      "500/500 [==============================] - 107s 215ms/step - loss: 0.5137\n",
      "Epoch 19/30\n",
      "500/500 [==============================] - 95s 189ms/step - loss: 0.5144\n",
      "Epoch 20/30\n",
      "500/500 [==============================] - 102s 205ms/step - loss: 0.5148\n",
      "Epoch 21/30\n",
      "500/500 [==============================] - 101s 203ms/step - loss: 0.5152\n",
      "Epoch 22/30\n",
      "500/500 [==============================] - 109s 217ms/step - loss: 0.5137\n",
      "Epoch 23/30\n",
      "500/500 [==============================] - 108s 216ms/step - loss: 0.5139\n",
      "Epoch 24/30\n",
      "500/500 [==============================] - 100s 199ms/step - loss: 0.5134\n",
      "Epoch 25/30\n",
      "500/500 [==============================] - 111s 223ms/step - loss: 0.5140\n",
      "Epoch 26/30\n",
      "500/500 [==============================] - 120s 239ms/step - loss: 0.5132\n",
      "Epoch 27/30\n",
      "500/500 [==============================] - 109s 219ms/step - loss: 0.5129\n",
      "Epoch 28/30\n",
      "500/500 [==============================] - 92s 184ms/step - loss: 0.5137\n",
      "Epoch 29/30\n",
      "500/500 [==============================] - 108s 216ms/step - loss: 0.5135\n",
      "Epoch 30/30\n",
      "500/500 [==============================] - 103s 206ms/step - loss: 0.5134\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0d5ba14fd0>"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    generator=generator_batch1(x_train, y_train),\n",
    "    steps_per_epoch=500,\n",
    "    epochs=30,\n",
    "    callbacks=[checkpointer]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Посчитаем метрики модели\n",
    "\n",
    "Данные очень несбалансированные, $\\sim 89 \\%$ данный вообще не содержат ни одной метки. Поэтому метрика точности (accuracy), которую я считаю, почти ни о чем не говорит. Так что я счиатаю roc auc, но и accuracy тоже, просто она мне нравится."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 26s, sys: 15.7 s, total: 4min 41s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "predictions = np.array(list(map(\n",
    "    lambda x: np.argmax(model.predict(np.array([x]))[0], axis=1),\n",
    "    x_test[:5000],\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic\n",
      "accuracy: 0.9654\n",
      "rocauc: 0.8417470812262904\n",
      "\n",
      "severe_toxic\n",
      "accuracy: 0.99\n",
      "rocauc: 0.7401201322216836\n",
      "\n",
      "obscene\n",
      "accuracy: 0.983\n",
      "rocauc: 0.8660177042351308\n",
      "\n",
      "threat\n",
      "accuracy: 0.997\n",
      "rocauc: 0.5\n",
      "\n",
      "insult\n",
      "accuracy: 0.9706\n",
      "rocauc: 0.7698664857762405\n",
      "\n",
      "identity_hate\n",
      "accuracy: 0.9902\n",
      "rocauc: 0.5\n",
      "\n",
      "toxic_content\n",
      "accuracy: 0.9642\n",
      "rocauc: 0.8486476022885473\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels_column = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'] + ['toxic_content']\n",
    "\n",
    "for i in range(7):\n",
    "    print(labels_column[i])\n",
    "    print('accuracy', end=': ')\n",
    "    print(accuracy_score(predictions[:, i], y_test[:5000, i]))\n",
    "    print('rocauc', end=': ')\n",
    "    print(roc_auc_score(y_test[:5000, i], predictions[:, i]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "предсказания по всему"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7237712865354133"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test[:5000], predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "без искуственно добавленной метки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7029585672432243"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test[:5000, :6], predictions[:, :6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Немного поанализируем модель\n",
    "\n",
    "\n",
    "Попытка сделать бонусное задание по интропретируемости.\n",
    "\n",
    "Посмотрим, на что модельно обращает внимание больше, а на что меньше. Для этого есть инересный подход, заменять последовательно слова на UNK и смотреть, как меняется оцнека моделью вероятностей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# всмомогательная функция рисования цветных слов\n",
    "# ПИСАЛ НЕ САМ! Взято из второго семинара курса ШАДа по NLP читавшегося осенью 2018 года.\n",
    "\n",
    "def draw_html(tokens_and_weights, cmap=plt.get_cmap(\"bwr\"), display=True,\n",
    "              token_template=\"\"\"<span style=\"background-color: {color_hex}\">{token}</span>\"\"\",\n",
    "              font_style=\"font-size:14px;\"\n",
    "             ):\n",
    "    \n",
    "    def get_color_hex(weight):\n",
    "        rgba = cmap(1. / (1 + np.exp(weight)), bytes=True)\n",
    "        return '#%02X%02X%02X' % rgba[:3]\n",
    "    \n",
    "    tokens_html = [\n",
    "        token_template.format(token=token, color_hex=get_color_hex(weight))\n",
    "        for token, weight in tokens_and_weights\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    raw_html = \"\"\"<p style=\"{}\">{}</p>\"\"\".format(font_style, ' '.join(tokens_html))\n",
    "    if display:\n",
    "        display_html(HTML(raw_html))\n",
    "        \n",
    "    return raw_html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь сам напишу функцию, которая будет считать вклады слов в итоговый скор.\n",
    "\n",
    "Функция принимает модель, токены и функцю извлечения нужной метрики из предсказания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wieght(model, tokens, weight_extracter):\n",
    "    # посчитает базовый скор, без унков\n",
    "    base_score = weight_extracter(model.predict(np.array([tokens]))[0])\n",
    "    \n",
    "    # сохраняем Токен унка\n",
    "    UNK_num = list(bpe_encoder.transform([bpe_encoder.UNK]))[0][0]\n",
    "    \n",
    "    # веса\n",
    "    weights = []\n",
    "    position = 0\n",
    "    \n",
    "    while position < len(tokens):\n",
    "        token = tokens[position]\n",
    "        \n",
    "        # если слово\n",
    "        if token in bpe_encoder.inverse_bpe_vocab.keys():\n",
    "            infer = process_tokenized_word(model, tokens, position, base_score, weight_extracter, UNK_num)\n",
    "        \n",
    "        # если нечто токенизированное\n",
    "        if token in bpe_encoder.inverse_word_vocab.keys():\n",
    "            infer = process_single_word(model, tokens, position, base_score, weight_extracter, UNK_num)\n",
    "            \n",
    "        weight, word, positino_up = infer\n",
    "        \n",
    "        weights.append((word, weight))\n",
    "        position += positino_up\n",
    "        \n",
    "    return weights, base_score\n",
    "\n",
    "def process_single_word(model, tokens, position, base_score, weight_extracter, UNK):\n",
    "    buf = tokens.copy()\n",
    "    buf[position] = UNK\n",
    "\n",
    "    # считаем вклад токена\n",
    "    predictions = model.predict(np.array([buf]))[0]\n",
    "    weight = weight_extracter(predictions) - base_score\n",
    "    \n",
    "    return (weight, bpe_encoder.inverse_word_vocab[tokens[position]], 1)\n",
    "\n",
    "def process_tokenized_word(model, tokens, position, base_score, weight_extracter, UNK):\n",
    "    # считаем вклад токенезированного слова как сумму вкладов его частей\n",
    "    sm = 0\n",
    "    for i in range(position, len(tokens)):\n",
    "        buf = tokens.copy()\n",
    "        buf[i] = UNK\n",
    "        \n",
    "        # считаем вклад токена\n",
    "        predictions = model.predict(np.array([buf]))[0]\n",
    "        weight = weight_extracter(predictions)\n",
    "        sm += weight - base_score\n",
    "        \n",
    "        # если токен был концом слова, вернем вес, само слово и новую позицию\n",
    "        if tokens[i] == bpe_encoder.bpe_vocab['__eow']:\n",
    "            word = next(bpe_encoder.inverse_transform([tokens[position : i + 1]]))\n",
    "            return (sm, word, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Понаслаждаемся, раскрасим некоторые предложения по влиянию на токсичность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drow_tokcik(tokens, realy=None):\n",
    "    tokens_and_weights, base_score = get_wieght(model, tokens, lambda x: x[0][1])\n",
    "    print(f'It was tixic for { str(round(base_score, 4))[:4] }')\n",
    "    if realy is not None:\n",
    "        print(f'And realy {realy[0]}')\n",
    "    draw_html([(tok, weight * 100) for tok, weight in tokens_and_weights]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was tixic for 0.58\n",
      "And realy 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"font-size:14px;\"><span style=\"background-color: #3C3CFF\">\".</span> <span style=\"background-color: #FFFCFC\">.</span> <span style=\"background-color: #FFE6E6\">it</span> <span style=\"background-color: #ECECFF\">was</span> <span style=\"background-color: #3E3EFF\">very</span> <span style=\"background-color: #FFFCFC\">constructive</span> <span style=\"background-color: #FF0101\">you</span> <span style=\"background-color: #FF2828\">are</span> <span style=\"background-color: #C8C8FF\">just</span> <span style=\"background-color: #1A1AFF\">very</span> <span style=\"background-color: #1010FF\">very</span> <span style=\"background-color: #FF0000\">stupid</span> <span style=\"background-color: #FFB3B3\">.</span> <span style=\"background-color: #1A1AFF\">arkjedinum</span></p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drow_tokcik(x_test[1], y_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was tixic for 0.81\n",
      "And realy 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"font-size:14px;\"><span style=\"background-color: #FFF2F2\">jonnum</span> <span style=\"background-color: #FFF6F6\"></span> <span style=\"background-color: #F6F6FF\">the</span> <span style=\"background-color: #FFF6F6\">biggest</span> <span style=\"background-color: #FF5454\">douchebag</span> <span style=\"background-color: #FFCACA\">u</span> <span style=\"background-color: #FFF2F2\">dont</span> <span style=\"background-color: #F8F8FF\">even</span> <span style=\"background-color: #FF9292\">kno</span></p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drow_tokcik(x_test[10], y_test[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was tixic for 0.21\n",
      "And realy 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"font-size:14px;\"><span style=\"background-color: #F2F2FF\">i</span> <span style=\"background-color: #CCCCFF\">linked</span> <span style=\"background-color: #FAFAFF\">onto</span> <span style=\"background-color: #FFF2F2\">a</span> <span style=\"background-color: #ECECFF\">page</span> <span style=\"background-color: #FEFEFF\">of</span> <span style=\"background-color: #FFFEFE\">my</span> <span style=\"background-color: #E3E3FF\">website</span> <span style=\"background-color: #FEFEFF\">just</span> <span style=\"background-color: #C8C8FF\">for</span> <span style=\"background-color: #FFDADA\">you</span> <span style=\"background-color: #2A2AFF\">:</span></p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drow_tokcik(x_test[100], y_test[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was tixic for 0.21\n",
      "And realy 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"font-size:14px;\"><span style=\"background-color: #D0D0FF\">where</span> <span style=\"background-color: #F3F3FF\">did</span> <span style=\"background-color: #FEFEFF\">i</span> <span style=\"background-color: #FEFEFF\">say</span> <span style=\"background-color: #FEFEFF\">the</span> <span style=\"background-color: #FFFEFE\">seminary</span> <span style=\"background-color: #FEFEFF\">?</span> <span style=\"background-color: #FFFEFE\">i</span> <span style=\"background-color: #FEFEFF\">said</span> <span style=\"background-color: #FEFEFF\">it</span> <span style=\"background-color: #FEFEFF\">wasn</span> <span style=\"background-color: #FFFEFE\">'</span> <span style=\"background-color: #FEFEFF\">t</span> <span style=\"background-color: #FFFEFE\">a</span> <span style=\"background-color: #FEFEFF\">significant</span> <span style=\"background-color: #FFFEFE\">enough</span> <span style=\"background-color: #FEFEFF\">post</span> <span style=\"background-color: #FEFEFF\">for</span> <span style=\"background-color: #FEFEFF\">the</span> <span style=\"background-color: #FFFEFE\">job</span> <span style=\"background-color: #FFFEFE\">holder</span> <span style=\"background-color: #FEFEFF\">to</span> <span style=\"background-color: #FEFEFF\">be</span> <span style=\"background-color: #FEFEFF\">inherently</span> <span style=\"background-color: #FEFEFF\">notable</span> <span style=\"background-color: #FEFEFF\">.</span> <span style=\"background-color: #FEFEFF\">arguments</span> <span style=\"background-color: #FFFEFE\">to</span> <span style=\"background-color: #FEFEFF\">the</span> <span style=\"background-color: #FFFAFA\">contrary</span> <span style=\"background-color: #FFFEFE\">need</span> <span style=\"background-color: #FEFEFF\">some</span> <span style=\"background-color: #FFFEFE\">form</span> <span style=\"background-color: #FFFEFE\">of</span> <span style=\"background-color: #FFFEFE\">policy</span> <span style=\"background-color: #FFFEFE\">basis</span> <span style=\"background-color: #FFFEFE\">so</span> <span style=\"background-color: #FFFEFE\">please</span> <span style=\"background-color: #FFFEFE\">feel</span> <span style=\"background-color: #FFFEFE\">free</span> <span style=\"background-color: #FFFEFE\">to</span> <span style=\"background-color: #FFFEFE\">provide</span> <span style=\"background-color: #FEFEFF\">that</span> <span style=\"background-color: #FFFEFE\">.</span> <span style=\"background-color: #FEFEFF\">as</span> <span style=\"background-color: #FFFEFE\">for</span> <span style=\"background-color: #FEFEFF\">the</span> <span style=\"background-color: #FEFEFF\">sources</span> <span style=\"background-color: #FEFEFF\">if</span> <span style=\"background-color: #FFFEFE\">they</span> <span style=\"background-color: #FFFEFE\">were</span> <span style=\"background-color: #FEFEFF\">present</span> <span style=\"background-color: #FFFEFE\">during</span> <span style=\"background-color: #FEFEFF\">the</span> <span style=\"background-color: #FEFEFF\">discussion</span> <span style=\"background-color: #FFFEFE\">and</span> <span style=\"background-color: #FEFEFF\">the</span> <span style=\"background-color: #FFFEFE\">neither</span> <span style=\"background-color: #FEFEFF\">the</span> <span style=\"background-color: #FFFEFE\">keep</span> <span style=\"background-color: #FFFEFE\">or</span> <span style=\"background-color: #FEFEFF\">delete</span> <span style=\"background-color: #FEFEFF\">side</span> <span style=\"background-color: #FEFEFF\">found</span> <span style=\"background-color: #FEFEFF\">them</span> <span style=\"background-color: #FEFEFF\">notable</span> <span style=\"background-color: #FFFEFE\">enough</span> <span style=\"background-color: #FEFEFF\">to</span> <span style=\"background-color: #FEFEFF\">discuss</span> <span style=\"background-color: #FEFEFF\">specifically</span> <span style=\"background-color: #FEFEFF\">then</span> <span style=\"background-color: #FEFEFF\">i</span> <span style=\"background-color: #FFFEFE\">don</span> <span style=\"background-color: #FEFEFF\">'</span> <span style=\"background-color: #FFFEFE\">t</span> <span style=\"background-color: #FEFEFF\">think</span> <span style=\"background-color: #FFFEFE\">i</span> <span style=\"background-color: #FEFEFF\">can</span> <span style=\"background-color: #FFFEFE\">disregard</span> <span style=\"background-color: #FEFEFF\">the</span> <span style=\"background-color: #FEFEFF\">discussion</span> <span style=\"background-color: #FEFEFF\">and</span> <span style=\"background-color: #FFFEFE\">form</span> <span style=\"background-color: #FFFEFE\">my</span> <span style=\"background-color: #FFFEFE\">own</span> <span style=\"background-color: #FEFEFF\">opinion</span> <span style=\"background-color: #FFFEFE\">of</span> <span style=\"background-color: #FEFEFF\">the</span> <span style=\"background-color: #FEFEFF\">sources</span> <span style=\"background-color: #FFFEFE\">.</span> <span style=\"background-color: #FEFEFF\">there</span> <span style=\"background-color: #FFFEFE\">is</span> <span style=\"background-color: #FFFCFC\">plenty</span> <span style=\"background-color: #FEFEFF\">of</span> <span style=\"background-color: #FAFAFF\">evidence</span> <span style=\"background-color: #FEFEFF\">in</span> <span style=\"background-color: #FEFEFF\">the</span> <span style=\"background-color: #F6F6FF\">discussion</span> <span style=\"background-color: #FEFEFF\">of</span> <span style=\"background-color: #FAFAFF\">the</span> <span style=\"background-color: #FCFCFF\">sources</span> <span style=\"background-color: #FCFCFF\">being</span> <span style=\"background-color: #FFF6F6\">thoroughly</span> <span style=\"background-color: #F3F3FF\">reviewed</span> <span style=\"background-color: #FFFCFC\">.</span> <span style=\"background-color: #FFD2D2\">humbug</span></p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drow_tokcik(x_test[186], y_test[186])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was tixic for 0.31\n",
      "And realy 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"font-size:14px;\"><span style=\"background-color: #7C7CFF\">\".</span> <span style=\"background-color: #FEFEFF\">.</span> <span style=\"background-color: #A8A8FF\">bektashi</span> <span style=\"background-color: #ECECFF\">i</span> <span style=\"background-color: #DEDEFF\">suspect</span> <span style=\"background-color: #FF2929\">you</span> <span style=\"background-color: #FFFCFC\">start</span> <span style=\"background-color: #FFBABA\">all</span> <span style=\"background-color: #FF9696\">your</span> <span style=\"background-color: #FEFEFF\">messages</span> <span style=\"background-color: #A2A2FF\">with</span> <span style=\"background-color: #FFA2A2\">offensive</span> <span style=\"background-color: #8080FF\">accusations</span> <span style=\"background-color: #FFAEAE\">like</span> <span style=\"background-color: #5858FF\">\"\"</span> <span style=\"background-color: #D3D3FF\">i</span> <span style=\"background-color: #FCFCFF\">suspect</span> <span style=\"background-color: #FF0909\">you</span> <span style=\"background-color: #AEAEFF\">'</span> <span style=\"background-color: #FF7070\">re</span> <span style=\"background-color: #FF6868\">a</span> <span style=\"background-color: #FF1919\">sock</span> <span style=\"background-color: #FF0808\">puppet</span> <span style=\"background-color: #7070FF\">of</span> <span style=\"background-color: #7070FF\">so</span> <span style=\"background-color: #7070FF\">-</span> <span style=\"background-color: #F6F6FF\">and</span> <span style=\"background-color: #CECEFF\">-</span> <span style=\"background-color: #EEEEFF\">so</span> <span style=\"background-color: #F6F6FF\">.\"\"</span> <span style=\"background-color: #F0F0FF\">that</span> <span style=\"background-color: #F8F8FF\">being</span> <span style=\"background-color: #F8F8FF\">said</span> <span style=\"background-color: #FFFAFA\">,</span> <span style=\"background-color: #FAFAFF\">no</span> <span style=\"background-color: #FFFAFA\">,</span> <span style=\"background-color: #FEFEFF\">i</span> <span style=\"background-color: #FEFEFF\">'</span> <span style=\"background-color: #FAFAFF\">m</span> <span style=\"background-color: #F2F2FF\">not</span> <span style=\"background-color: #FFF4F4\">a</span> <span style=\"background-color: #FFECEC\">sock</span> <span style=\"background-color: #FEFEFF\">-</span> <span style=\"background-color: #FFE6E6\">puppet</span> <span style=\"background-color: #F8F8FF\">of</span> <span style=\"background-color: #FCFCFF\">aldiramal</span> <span style=\"background-color: #FEFEFF\">jokes</span> <span style=\"background-color: #FEFEFF\">,</span> <span style=\"background-color: #FFFEFE\">and</span> <span style=\"background-color: #FFFEFE\">including</span> <span style=\"background-color: #FFFEFE\">one</span> <span style=\"background-color: #FFFEFE\">joke</span> <span style=\"background-color: #FEFEFF\">,</span> <span style=\"background-color: #FFFEFE\">would</span> <span style=\"background-color: #FFFEFE\">seem</span> <span style=\"background-color: #B6B6FF\">fair</span> <span style=\"background-color: #FEFEFF\">.</span> <span style=\"background-color: #FFFEFE\">happy</span> <span style=\"background-color: #FFFEFE\">editing</span> <span style=\"background-color: #FFF2F2\">to</span> <span style=\"background-color: #FFF8F8\">you</span> <span style=\"background-color: #F0F0FF\">too</span> <span style=\"background-color: #FFFEFE\">.</span> <span style=\"background-color: #FFFEFE\">\"</span></p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drow_tokcik(x_test[86], y_test[86])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was tixic for 0.81\n",
      "And realy 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"font-size:14px;\"><span style=\"background-color: #FFFEFE\">\"</span> <span style=\"background-color: #FFFEFE\">dude</span> <span style=\"background-color: #FFFEFE\">,</span> <span style=\"background-color: #FEFEFF\">i</span> <span style=\"background-color: #FEFEFF\">'</span> <span style=\"background-color: #FFFEFE\">m</span> <span style=\"background-color: #F3F3FF\">not</span> <span style=\"background-color: #FFFAFA\">angry</span> <span style=\"background-color: #F6F6FF\">at</span> <span style=\"background-color: #FFFCFC\">you</span> <span style=\"background-color: #FFFEFE\">,</span> <span style=\"background-color: #FEFEFF\">i</span> <span style=\"background-color: #FEFEFF\">just</span> <span style=\"background-color: #F6F6FF\">think</span> <span style=\"background-color: #FFF4F4\">you</span> <span style=\"background-color: #FFFEFE\">'</span> <span style=\"background-color: #F0F0FF\">re</span> <span style=\"background-color: #FFFCFC\">a</span> <span style=\"background-color: #FF6C6C\">stupid</span> <span style=\"background-color: #FF5858\">fuckwit</span> <span style=\"background-color: #FEFEFF\">,</span> <span style=\"background-color: #FCFCFF\">thorough</span> <span style=\"background-color: #FEFEFF\">rim</span> <span style=\"background-color: #FFB6B6\">fuck</span> <span style=\"background-color: #FCFCFF\">if</span> <span style=\"background-color: #FFEEEE\">you</span> <span style=\"background-color: #E8E8FF\">actually</span> <span style=\"background-color: #F8F8FF\">think</span> <span style=\"background-color: #FEFEFF\">the</span> <span style=\"background-color: #DADAFF\">democratic</span> <span style=\"background-color: #FFFCFC\">party</span> <span style=\"background-color: #FFFEFE\">is</span> <span style=\"background-color: #FEFEFF\">even</span> <span style=\"background-color: #FEFEFF\">worth</span> <span style=\"background-color: #F6F6FF\">wiping</span></p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drow_tokcik(x_test[25], y_test[25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иногда модель обращает внимание на какой-то бред :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Экспеементы быстродействия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./data/train.csv')\n",
    "texts = list(train_data.comment_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_prediction(text_line, model):\n",
    "    tokens = next(bpe_encoder.transform([text_line]))\n",
    "    prediction = np.argmax(model.predict(np.array([tokens]))[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 34min 11s, sys: 5min 52s, total: 1h 40min 3s\n",
      "Wall time: 31min 3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " ...]"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "list(map(lambda x : get_single_prediction(x, model), texts[:100 * 1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это заняло $94$ минуты процессорного времени. То есть $57ms$ на полный процессинг одного текста. Что кажется довольно быстро, учитывая возможность процессить батчами.\n",
    "\n",
    "*Во время всего обучения и процессингов использовались CPU: 4 ядра Intel Core5*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05651"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(94 * 60 + 11) / 100 / 1000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
